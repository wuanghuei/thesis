import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from model_fixed import TemporalActionDetector
from dataloader import get_train_loader, get_val_loader, get_test_loader
from sklearn.metrics import precision_recall_fscore_support, average_precision_score
from tqdm import tqdm
import numpy as np
import os
import json
import matplotlib.pyplot as plt
from datetime import datetime
from torch.cuda.amp import autocast, GradScaler
from collections import defaultdict


# ====== Config ======
NUM_CLASSES = 5  # ƒê·∫£m b·∫£o kh·ªõp v·ªõi ƒë·ªãnh nghƒ©a trong prepare_segments.py v√† dataloader.py
WINDOW_SIZE = 32  # K√≠ch th∆∞·ªõc sliding window, ph·∫£i kh·ªõp v·ªõi WINDOW_SIZE trong dataloader.py
EPOCHS = 100
BATCH_SIZE = 1
GRADIENT_ACCUMULATION_STEPS = 4
LR = 1e-5  # Gi·∫£m t·ª´ 5e-5 xu·ªëng 2e-5 ƒë·ªÉ ·ªïn ƒë·ªãnh loss
WEIGHT_DECAY = 1e-4  # TƒÉng weight decay ƒë·ªÉ tr√°nh overfitting
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
CHECKPOINT_DIR = "checkpoints"
CHECKPOINT = os.path.join(CHECKPOINT_DIR, "best_model_velocity.pth")  # D√πng checkpoint m·ªõi cho phi√™n b·∫£n v·ªõi velocity
RESUME_CHECKPOINT = os.path.join(CHECKPOINT_DIR, "interim_model_epoch15.pth")  # D√πng checkpoint m·ªõi cho phi√™n b·∫£n v·ªõi velocity
LOG_DIR = "logs"
USE_MIXED_PRECISION = True
RESUME_TRAINING = True  # Continue from checkpoint
BOUNDARY_THRESHOLD = 0.11  # Gi·∫£m t·ª´ 0.15 xu·ªëng 0.08
DEBUG_DETECTION = True  # Enable detection debugging
MAX_GRAD_NORM = 7.0  # TƒÉng t·ª´ 1.0 l√™n 5.0 ƒë·ªÉ ·ªïn ƒë·ªãnh gradient
FINAL_EVALUATION = True  # Ch·∫°y ƒë√°nh gi√° cu·ªëi c√πng tr√™n test set
WARMUP_EPOCHS = 7  # Gi·∫£m s·ªë epoch cho learning rate warmup t·ª´ 5 xu·ªëng 3
WARMUP_FACTOR = 2.5  # TƒÉng LR trong warmup l√™n 2.5x (tƒÉng t·ª´ 2.0)

# S·ª≠ d·ª•ng threshold ri√™ng cho t·ª´ng l·ªõp - ƒëi·ªÅu ch·ªânh theo ƒë·∫∑c t√≠nh c·ªßa t·ª´ng action class
CLASS_THRESHOLDS = [0.15, 0.15, 0.01, 0.08, 0.15]  # Gi·∫£m thresholds ƒë·ªÉ ph√π h·ª£p v·ªõi ph√¢n ph·ªëi x√°c su·∫•t hi·ªán t·∫°i

# Tr·ªçng s·ªë cho loss components - ƒëi·ªÅu ch·ªânh ƒë·ªÉ t·∫≠p trung m·∫°nh h∆°n v√†o action classification
ACTION_WEIGHT = 1.5  # TƒÉng t·ª´ 2.0 l√™n 3.0 ƒë·ªÉ t·∫≠p trung h∆°n v√†o action classification
START_WEIGHT = 1.5  # Gi·∫£m t·ª´ 1.5 xu·ªëng 1.0
END_WEIGHT = 1.5  # Gi·∫£m t·ª´ 1.5 xu·ªëng 1.0

# ====== TH√äM CONFIG CHO POST-PROCESSING ======
MIN_SEGMENT_LENGTH = 3  # ƒê·ªô d√†i t·ªëi thi·ªÉu c·ªßa segment (frames) - gi·∫£m t·ª´ 10 xu·ªëng 8
MIN_CONFIDENT_RATIO = 0.15  # Gi·∫£m t·ª´ 0.2 xu·ªëng 0.15 ƒë·ªÉ cho ph√©p nhi·ªÅu detections h∆°n
NMS_THRESHOLD = 0.4  # TƒÉng t·ª´ 0.3 l√™n 0.5

# T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


def set_seed(seed=42):
    """ƒê·∫∑t seed cho t·∫•t c·∫£ c√°c generator ng·∫´u nhi√™n ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh t√°i l·∫≠p"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # if you use multi-GPU
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    print(f"ƒê√£ thi·∫øt l·∫≠p seed={seed} cho t·∫•t c·∫£ generator ng·∫´u nhi√™n")

# ====== Debug Functions ======
def debug_detection_stats(batch_detections, batch_size, metadata):
    """Print detection statistics for debugging"""
    total_dets = sum(len(dets) for dets in batch_detections)
    if total_dets == 0:
        print("‚ö†Ô∏è WARNING: No detections in batch!")
        return
    
    print(f"Detections in batch: {total_dets} (avg {total_dets/batch_size:.1f} per sample)")
    
    # Count detections per class
    class_counts = {}
    for i, dets in enumerate(batch_detections):
        video_id = metadata[i]["video_id"] if i < len(metadata) else "unknown"
        print(f"  Sample {i} (video {video_id}): {len(dets)} detections")
        
        for det in dets:
            action_id = det["action_id"]
            if action_id not in class_counts:
                class_counts[action_id] = 0
            class_counts[action_id] += 1
    
    # Print class statistics
    for action_id, count in sorted(class_counts.items()):
        print(f"  Class {action_id}: {count} detections")
    
    # Print detection details for first few detections
    if total_dets > 0:
        print("\nDetection details (first 3):")
        count = 0
        for i, dets in enumerate(batch_detections):
            if len(dets) > 0:
                for det in dets[:min(3, len(dets))]:
                    print(f"  Det {count}: Class {det['action_id']}, Start: {det['start_frame']}, End: {det['end_frame']}, Conf: {det['confidence']:.4f}")
                    count += 1
                    if count >= 3:
                        break
            if count >= 3:
                break

def debug_raw_predictions(action_probs, start_probs, end_probs):
    """Analyze raw prediction values before thresholding"""
    # Check variance in predictions (helpful to detect potential collapse)
    action_variance = torch.var(action_probs).item()
    print(f"Action prediction variance: {action_variance:.6f}")
    
    # Check per-class stats
    for c in range(action_probs.shape[2]):  # For each class
        class_probs = action_probs[:, :, c]
        print(f"  Class {c}: min={class_probs.min().item():.4f}, max={class_probs.max().item():.4f}, mean={class_probs.mean().item():.4f}", end = " - ")
    print("\n")

# ====== Loss Functions ======
class ActionDetectionLoss(nn.Module):
    def __init__(self, action_weight=ACTION_WEIGHT, start_weight=START_WEIGHT, end_weight=END_WEIGHT, label_smoothing=0.1):
        """
        Loss function for temporal action detection
        
        Args:
            action_weight: Tr·ªçng s·ªë cho action segmentation loss
            start_weight: Tr·ªçng s·ªë cho start point detection loss
            end_weight: Tr·ªçng s·ªë cho end point detection loss
            label_smoothing: Amount of label smoothing to apply (0.0 to 0.5)
        """
        super().__init__()
        self.action_weight = action_weight
        self.start_weight = start_weight
        self.end_weight = end_weight
        self.label_smoothing = label_smoothing
        
        # BCE for action segmentation
        self.action_criterion = nn.BCEWithLogitsLoss(reduction='none')
        
        # BCE for start/end detection
        self.boundary_criterion = nn.BCEWithLogitsLoss(reduction='none')
        
        # Tr·ªçng s·ªë cho t·ª´ng l·ªõp (tƒÉng tr·ªçng s·ªë cho Class 2 v√† 3)
        self.class_weights = torch.ones(NUM_CLASSES, device=DEVICE)
        self.class_weights[0] = 1.0 
        self.class_weights[1] = 1.5 
        self.class_weights[2] = 7.0  # TƒÉng m·∫°nh t·ª´ 3.3 l√™n 7.0
        self.class_weights[3] = 2.0  # Th√™m tr·ªçng s·ªë cho Class 3
        self.class_weights[4] = 1.0 

        # Tr·ªçng s·ªë ri√™ng cho boundary loss c·ªßa t·ª´ng l·ªõp
        self.boundary_weights = torch.ones(NUM_CLASSES, device=DEVICE)
        # TƒÉng m·∫°nh tr·ªçng s·ªë boundary cho Class 2
        self.boundary_weights[2] = 5.0  # Th√™m tr·ªçng s·ªë ƒë√°ng k·ªÉ cho boundary Class 2

    def smooth_labels(self, targets):
        """Apply label smoothing to targets"""
        if self.label_smoothing <= 0:
            return targets
        return targets * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing
        
    def forward(self, predictions, targets):
        """
        Calculate loss for temporal action detection
        
        Args:
            predictions: dict v·ªõi keys 'action_scores', 'start_scores', 'end_scores'
                  - action_scores: (B, T, C) - scores cho m·ªói frame v√† class
                  - start_scores: (B, T, C) - scores cho start points
                  - end_scores: (B, T, C) - scores cho end points
                  
            targets: dict v·ªõi keys 'action_masks', 'start_masks', 'end_masks'
                  - action_masks: (B, C, T) - binary masks cho actions
                  - start_masks: (B, C, T) - Gaussian smoothed masks cho start points
                  - end_masks: (B, C, T) - Gaussian smoothed masks cho end points
            
        Returns:
            Dict v·ªõi 'total' loss v√† c√°c individual components
        """
        action_scores = predictions['action_scores']  # (B, T, C)
        start_scores = predictions['start_scores']    # (B, T, C)
        end_scores = predictions['end_scores']        # (B, T, C)
        
        action_masks = targets['action_masks']  # (B, C, T)
        start_masks = targets['start_masks']    # (B, C, T) - ƒê√£ √°p d·ª•ng Gaussian smoothing trong dataloader
        end_masks = targets['end_masks']        # (B, C, T) - ƒê√£ √°p d·ª•ng Gaussian smoothing trong dataloader
        
        # Transpose targets to match predictions
        action_masks = action_masks.transpose(1, 2)  # (B, T, C)
        start_masks = start_masks.transpose(1, 2)    # (B, T, C)
        end_masks = end_masks.transpose(1, 2)        # (B, T, C)
        
        # Apply label smoothing if enabled
        if self.label_smoothing > 0:
            action_masks = self.smooth_labels(action_masks)
            start_masks = self.smooth_labels(start_masks)
            end_masks = self.smooth_labels(end_masks)
        
        # Calculate action loss v·ªõi class weights
        action_loss = self.action_criterion(action_scores, action_masks)
        
        # √Åp d·ª•ng class weights
        action_loss = action_loss * self.class_weights.view(1, 1, -1)
        action_loss = action_loss.mean()
        
        # Only consider regions with actions for start/end loss
        # T·∫°o mask cho nh·ªØng frame c√≥ √≠t nh·∫•t m·ªôt action
        valid_regions = (action_masks.sum(dim=2, keepdim=True) > 0).float()
        
        # Calculate start/end loss v·ªõi class weights
        start_loss = self.boundary_criterion(start_scores, start_masks)
        end_loss = self.boundary_criterion(end_scores, end_masks)
        
        # √Åp d·ª•ng class weights cho start/end loss
        start_loss = start_loss * self.boundary_weights.view(1, 1, -1) # S·ª≠ d·ª•ng boundary_weights
        end_loss = end_loss * self.boundary_weights.view(1, 1, -1)   # S·ª≠ d·ª•ng boundary_weights
        
        # Apply valid regions mask and normalize
        if valid_regions.sum() > 0:
            # Ch·ªâ t√≠nh loss tr√™n nh·ªØng frame c√≥ action
            start_loss = (start_loss * valid_regions).sum() / (valid_regions.sum() + 1e-6)
            end_loss = (end_loss * valid_regions).sum() / (valid_regions.sum() + 1e-6)
        else:
            # Fallback khi kh√¥ng c√≥ frame n√†o c√≥ action
            start_loss = start_loss.mean()
            end_loss = end_loss.mean()
        
        # Total loss v·ªõi weighted components
        total_loss = (
            self.action_weight * action_loss + 
            self.start_weight * start_loss + 
            self.end_weight * end_loss
        )
        
        return {
            'total': total_loss,
            'action': action_loss.detach(),
            'start': start_loss.detach(),
            'end': end_loss.detach()
        }

def calculate_f1_at_iou(gt_segments, pred_segments, iou_threshold):
    """Calculate F1 score at a specific IoU threshold"""
    if not pred_segments:
        return 0.0, 0.0, 0.0  # Precision, Recall, F1
    
    # S·∫Øp x·∫øp predictions theo confidence
    pred_segments = sorted(pred_segments, key=lambda x: x['score'], reverse=True)
    
    true_positives = 0
    gt_matched = [False] * len(gt_segments)
    
    for pred in pred_segments:
        pred_segment = pred['segment']
        best_iou = 0
        best_idx = -1
        
        # T√¨m GT c√≥ IoU cao nh·∫•t v·ªõi prediction n√†y
        for i, gt_segment in enumerate(gt_segments):
            if not gt_matched[i]:  # Ch·ªâ x√©t GT ch∆∞a ƒë∆∞·ª£c match
                iou = calculate_temporal_iou(pred_segment, gt_segment)
                if iou > best_iou:
                    best_iou = iou
                    best_idx = i
        
        # N·∫øu IoU >= threshold, ƒë√°nh d·∫•u l√† true positive
        if best_iou >= iou_threshold and best_idx >= 0:
            true_positives += 1
            gt_matched[best_idx] = True
    
    # T√≠nh precision, recall, F1
    precision = true_positives / len(pred_segments) if pred_segments else 0
    recall = true_positives / len(gt_segments) if gt_segments else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    return precision, recall, f1

def resolve_cross_class_overlaps(merged_detections):
    """Gi·∫£i quy·∫øt ch·ªìng l·∫•p gi·ªØa c√°c l·ªõp sau khi merge, kh√¥ng cho ph√©p b·∫•t k·ª≥ frame n√†o b·ªã ch·ªìng l·∫•n"""
    for video_id, detections in merged_detections.items():
        # S·∫Øp x·∫øp theo confidence gi·∫£m d·∫ßn
        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
        
        # Kh·ªüi t·∫°o m·∫£ng detections m·ªõi kh√¥ng c√≥ ch·ªìng l·∫•p
        final_detections = []
        
        # T√¨m frame cu·ªëi c√πng trong t·∫•t c·∫£ detections
        max_frame = max([det['end_frame'] for det in detections]) if detections else 0
        frames_occupied = [False] * (max_frame + 1)
        
        for det in detections:
            start = det['start_frame']
            end = det['end_frame']
            
            # Ki·ªÉm tra overlap - n·∫øu b·∫•t k·ª≥ frame n√†o ƒë√£ b·ªã chi·∫øm, b·ªè qua detection n√†y
            overlap = False
            for t in range(start, end):
                if t < len(frames_occupied) and frames_occupied[t]:
                    overlap = True
                    break
            
            if not overlap:
                # Th√™m detection v√† ƒë√°nh d·∫•u t·∫•t c·∫£ frame c·ªßa n√≥ l√† ƒë√£ chi·∫øm
                for t in range(start, end):
                    if t < len(frames_occupied):
                        frames_occupied[t] = True
                final_detections.append(det)
        
        # C·∫≠p nh·∫≠t l·∫°i danh s√°ch detections cho video n√†y
        merged_detections[video_id] = final_detections
    
    return merged_detections

def merge_cross_window_detections(all_window_detections, all_window_metadata, iou_threshold=0.2, confidence_threshold=0.15):
    """
    K·∫øt h·ª£p detections t·ª´ c√°c sliding window ch·ªìng l·∫•p
    
    Args:
        all_window_detections: List c√°c detections t·ª´ m·ªói window [window_idx][detection_idx]
        all_window_metadata: Th√¥ng tin v·ªÅ m·ªói window (video_id, start_idx, end_idx)
        iou_threshold: Ng∆∞·ª°ng IoU ƒë·ªÉ k·∫øt h·ª£p c√°c detections li√™n ti·∫øp
        confidence_threshold: Ng∆∞·ª°ng confidence ƒë·ªÉ ch·∫•p nh·∫≠n k·∫øt h·ª£p
        
    Returns:
        merged_detections: Danh s√°ch c√°c detections ƒë√£ k·∫øt h·ª£p xuy√™n window
    """
    # T·ªï ch·ª©c c√°c detections theo video_id v√† action_id
    video_detections = defaultdict(lambda: defaultdict(list))
    
    for window_idx, (window_dets, meta) in enumerate(zip(all_window_detections, all_window_metadata)):
        video_id = meta['video_id']
        start_idx = meta['start_idx']  # V·ªã tr√≠ b·∫Øt ƒë·∫ßu c·ªßa window trong video
        
        for det in window_dets:
            action_id = det['action_id']
            # Chuy·ªÉn coordinates t·ª´ window-relative sang global video coordinates
            global_start = start_idx + det['start_frame']
            global_end = start_idx + det['end_frame']
            confidence = det['confidence']
            
            video_detections[video_id][action_id].append({
                'start_frame': global_start,
                'end_frame': global_end,
                'confidence': confidence,
                'window_idx': window_idx
            })
    
    # K·∫øt h·ª£p c√°c detections thu·ªôc c√πng m·ªôt action trong m·ªói video
    merged_results = {}
    for video_id, action_dets in video_detections.items():
        merged_results[video_id] = []
        
        for action_id, dets in action_dets.items():
            # S·∫Øp x·∫øp theo v·ªã tr√≠ b·∫Øt ƒë·∫ßu
            dets = sorted(dets, key=lambda x: x['start_frame'])
            
            # K·∫øt h·ª£p c√°c detections b·ªã ng·∫Øt do window size
            i = 0
            while i < len(dets):
                current = dets[i]
                merged = dict(current)  # Copy ƒë·ªÉ kh√¥ng thay ƒë·ªïi detection g·ªëc
                
                j = i + 1
                while j < len(dets):
                    next_det = dets[j]
                    
                    # Ki·ªÉm tra xem hai detections c√≥ kh·∫£ nƒÉng l√† m·ªôt h√†nh ƒë·ªông b·ªã c·∫Øt kh√¥ng
                    overlap = min(merged['end_frame'], next_det['end_frame']) - max(merged['start_frame'], next_det['start_frame'])
                    overlap_ratio = overlap / min(merged['end_frame'] - merged['start_frame'], next_det['end_frame'] - next_det['start_frame'])
                    
                    time_diff = abs(next_det['start_frame'] - merged['end_frame'])
                    
                    # ƒêi·ªÅu ki·ªán ƒë·ªÉ k·∫øt h·ª£p: c√≥ overlap ho·∫∑c c√°ch nhau kh√¥ng qu√° xa
                    if (overlap_ratio >= iou_threshold or time_diff <= 5) and \
                       (merged['confidence'] + next_det['confidence']) / 2 >= confidence_threshold:
                        # M·ªü r·ªông detection hi·ªán t·∫°i
                        merged['start_frame'] = min(merged['start_frame'], next_det['start_frame'])
                        merged['end_frame'] = max(merged['end_frame'], next_det['end_frame'])
                        merged['confidence'] = (merged['confidence'] * (merged['end_frame'] - merged['start_frame']) + 
                                             next_det['confidence'] * (next_det['end_frame'] - next_det['start_frame'])) / \
                                             ((merged['end_frame'] - merged['start_frame']) + 
                                             (next_det['end_frame'] - next_det['start_frame']))
                        dets.pop(j)  # Lo·∫°i b·ªè detection ƒë√£ k·∫øt h·ª£p
                    else:
                        j += 1
                
                merged_results[video_id].append({
                    'action_id': action_id,
                    'start_frame': merged['start_frame'],
                    'end_frame': merged['end_frame'],
                    'confidence': merged['confidence']
                })
                
                i += 1
    
    return merged_results

# ====== T√≠nh IoU between prediction and ground truth ======
def calculate_temporal_iou(pred_segment, gt_segment):
    """Calculate temporal IoU between prediction and ground truth segments"""
    pred_start, pred_end = pred_segment
    gt_start, gt_end = gt_segment
    
    # ƒê·∫£m b·∫£o end > start cho c·∫£ predicted v√† ground truth segments
    pred_start, pred_end = min(pred_start, pred_end), max(pred_start, pred_end)
    gt_start, gt_end = min(gt_start, gt_end), max(gt_start, gt_end)
    
    # Ki·ªÉm tra tr∆∞·ªùng h·ª£p segments b·ªã tho√°i h√≥a
    if pred_start == pred_end or gt_start == gt_end:
        return 0.0
    
    # Calculate intersection
    intersection = max(0, min(pred_end, gt_end) - max(pred_start, gt_start))
    
    # Calculate union
    union = max(1, (pred_end - pred_start) + (gt_end - gt_start) - intersection)
    
    return intersection / union

# ====== Training Function ======
def train(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, device, start_epoch=0, best_map=0):
    """Train the model"""
    initial_action_weight = ACTION_WEIGHT  # L∆∞u tr·ªçng s·ªë ban ƒë·∫ßu
    initial_start_weight = START_WEIGHT
    initial_end_weight = END_WEIGHT
    start_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(LOG_DIR, f"training_log_fixed_{start_time}.csv")
    
    # Write header to log file - ADDED NEW METRICS
    with open(log_file, 'w') as f:
        f.write("epoch,train_loss,val_loss,val_map,val_f1,map_mid,f1_iou_01,f1_iou_025,f1_iou_05,class0_ap,class1_ap,class2_ap,class3_ap,class4_ap\n")
    
    losses = {'train': [], 'val': []}
    maps = []
    class_aps = {c: [] for c in range(NUM_CLASSES)}
    
    # Kh·ªüi t·∫°o GradScaler cho mixed precision
    scaler = GradScaler(enabled=USE_MIXED_PRECISION)
    
    for epoch in range(start_epoch, epochs):
        if epoch >= 30:  # TƒÉng t·ª´ 20 l√™n 30 epochs ƒë·ªÉ t·∫≠p trung v√†o action classification l√¢u h∆°n
            # Gradually transition to more balanced weights
            progress = min(1.0, (epoch - 30) / 20)  # Transition over 20 epochs 
            criterion.action_weight = initial_action_weight * (1 - 0.3 * progress)  # Gi·∫£m d·∫ßn 30%
            criterion.start_weight = initial_start_weight * (1 + 0.5 * progress)  # TƒÉng d·∫ßn 50%
            criterion.end_weight = initial_end_weight * (1 + 0.5 * progress)  # TƒÉng d·∫ßn 50%
            print(f"Epoch {epoch+1}: Adjusted weights - Action: {criterion.action_weight:.2f}, Start: {criterion.start_weight:.2f}, End: {criterion.end_weight:.2f}")
        # Training
        model.train()
        train_loss = 0
        
        # Reset gradient accumulation
        optimizer.zero_grad()
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        batch_count = 0
        
        # Theo d√µi gradient
        grad_norms = []
        
        # Apply learning rate warmup if in warmup phase
        if epoch < WARMUP_EPOCHS:
            warmup_start = LR/2.5
            current_lr = warmup_start + (LR - warmup_start) * (epoch + 1) / WARMUP_EPOCHS
            for param_group in optimizer.param_groups:
                param_group['lr'] = current_lr
            print(f"Warmup LR: {current_lr:.8f}")
        
        for batch_idx, batch in enumerate(progress_bar):
            # Unpack the batch with RGB and Pose+Velocity streams
            frames, pose_data, hand_data, action_masks, start_masks, end_masks, _ = batch
            
            frames = frames.to(device)
            pose_data = pose_data.to(device)
            action_masks = action_masks.to(device)
            start_masks = start_masks.to(device)
            end_masks = end_masks.to(device)
            


            
            # Forward pass with mixed precision
            with autocast(enabled=USE_MIXED_PRECISION):
                # Forward pass v·ªõi RGB v√† Pose+Velocity
                predictions = model(frames, pose_data)
                
                # Debug raw predictions
                if batch_idx % 100 == 0:
                    # For classification model, adjust debugging
                    if 'classification' in predictions:
                        print(f"Classification logits: min={predictions['classification'].min().item():.4f}, max={predictions['classification'].max().item():.4f}")
                    else:
                        action_logits = predictions['action_scores']
                        print("\n")
                        print(f"Action logits: min={action_logits.min().item():.4f}, max={action_logits.max().item():.4f}")
                
                # Calculate loss
                targets = {
                    'action_masks': action_masks,
                    'start_masks': start_masks,
                    'end_masks': end_masks
                }
                
                loss_dict = criterion(predictions, targets)
                loss = loss_dict['total']
                
                # Normalize loss for gradient accumulation
                loss = loss / GRADIENT_ACCUMULATION_STEPS
            
            # Backward pass with gradient scaling
            scaler.scale(loss).backward()
            
            # Update metrics
            train_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS
            batch_count += 1
            
            # Gradient accumulation
            if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or (batch_idx + 1) == len(train_loader):
                # Unscale gradients before clipping
                scaler.unscale_(optimizer)
                
                # Gradient clipping ƒë·ªÉ ngƒÉn ch·∫∑n gradient explosion
                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=MAX_GRAD_NORM)
                grad_norms.append(grad_norm.item())
                
                # Update weights
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                
                # Update progress bar
                progress_bar.set_postfix({
                    'loss': f"{loss_dict['total'].item():.4f}",
                    'action': f"{loss_dict['action'].item():.4f}",
                    'start': f"{loss_dict['start'].item():.4f}",
                    'end': f"{loss_dict['end'].item():.4f}",
                    'grad': f"{grad_norm:.2f}"
                })
        
        # Print gradient statistics
        if grad_norms:
            print(f"Gradient stats: min={min(grad_norms):.4f}, max={max(grad_norms):.4f}, mean={np.mean(grad_norms):.4f}")
        
        # Average loss
        train_loss /= batch_count
        losses['train'].append(train_loss)
        
        
        
        # Validation - UPDATED to receive metrics dict
        val_metrics = evaluate(model, val_loader, criterion, device)
        val_loss = val_metrics['val_loss']
        val_map = val_metrics['mAP']
        val_f1 = val_metrics['merged_f1']
        class_ap_dict = val_metrics['class_aps']
        map_mid = val_metrics['map_mid']
        avg_f1_iou_01 = val_metrics['avg_f1_iou_010']
        avg_f1_iou_025 = val_metrics['avg_f1_iou_025']
        avg_f1_iou_05 = val_metrics['avg_f1_iou_050']
        
        losses['val'].append(val_loss)
        maps.append(val_map)
        
        # Update learning rate if not in warmup phase
        if epoch >= WARMUP_EPOCHS:
            # C·∫≠p nh·∫≠t scheduler v·ªõi validation loss
            scheduler.step(val_loss)
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Current LR: {current_lr:.8f}")
        
        # L∆∞u AP c·ªßa t·ª´ng l·ªõp
        for c in range(NUM_CLASSES):
            class_aps[c].append(class_ap_dict[c])
        
        # Log results - UPDATED to print new metrics
        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val mAP: {val_map:.4f}, Val F1: {val_f1:.4f}")
        print(f"  Extra Metrics: mAP@mid={map_mid:.4f}, F1@0.1={avg_f1_iou_01:.4f}, F1@0.25={avg_f1_iou_025:.4f}, F1@0.5={avg_f1_iou_05:.4f}")
        print(f"  Class AP: {', '.join([f'C{c}={class_ap_dict[c]:.4f}' for c in range(NUM_CLASSES)])}")
        
        # Write to log file - UPDATED to write new metrics
        with open(log_file, 'a') as f:
            f.write(f"{epoch+1},{train_loss},{val_loss},{val_map},{val_f1},{map_mid},{avg_f1_iou_01},{avg_f1_iou_025},{avg_f1_iou_05}")
            for c in range(NUM_CLASSES):
                f.write(f",{class_ap_dict[c]}")
            f.write("\n")
        
        # Save best model
        if val_map > best_map:
            best_map = val_map
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_map': val_map,
                'val_f1': val_f1,
                'class_aps': class_ap_dict
            }, CHECKPOINT)
            print(f"‚úÖ Saved best model with mAP: {val_map:.4f}")
            
            # Checkpoint filename with epoch and mAP
            epoch_checkpoint = os.path.join(CHECKPOINT_DIR, f"model_fixed_epoch{epoch+1}_map{val_map:.4f}.pth")
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_map': val_map,
                'val_f1': val_f1,
                'class_aps': class_ap_dict
            }, epoch_checkpoint)
        
        # L∆∞u checkpoint ƒë·ªãnh k·ª≥ m·ªói 5 epochs ho·∫∑c khi c√≥ detections ƒë·∫ßu ti√™n
        save_interim = False
        if (epoch + 1) % 1 == 0:  # L∆∞u m·ªói 5 epochs
            save_interim = True
            print(f"üíæ L∆∞u checkpoint ƒë·ªãnh k·ª≥ t·∫°i epoch {epoch+1}")
        elif val_map > 0 and best_map == val_map:  # L∆∞u khi c√≥ detection ƒë·∫ßu ti√™n
            save_interim = True
            print(f"üîç L∆∞u checkpoint khi c√≥ detection ƒë·∫ßu ti√™n: mAP = {val_map:.4f}")
        
        if save_interim:
            interim_checkpoint = os.path.join(CHECKPOINT_DIR, f"interim_model_epoch{epoch+1}.pth")
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_map': val_map,
                'val_f1': val_f1,
                'class_aps': class_ap_dict
            }, interim_checkpoint)
    
    # Plot training curves
    plt.figure(figsize=(15, 10))
    
    plt.subplot(2, 2, 1)
    plt.plot(losses['train'], label='Train Loss')
    plt.plot(losses['val'], label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Training and Validation Loss')
    
    plt.subplot(2, 2, 2)
    plt.plot(maps, label='Val mAP')
    plt.xlabel('Epoch')
    plt.ylabel('mAP')
    plt.title('Validation mAP')
    plt.legend()
    
    plt.subplot(2, 2, 3)
    for c in range(NUM_CLASSES):
        plt.plot(class_aps[c], label=f'Class {c}')
    plt.xlabel('Epoch')
    plt.ylabel('AP')
    plt.title('Class AP')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(LOG_DIR, f"training_curves_fixed_{start_time}.png"))
    
    return best_map

# ====== Evaluation Function ======
def evaluate(model, val_loader, criterion, device):
    """Evaluate the model"""
    model.eval()
    val_loss = 0

    # L∆∞u t·∫•t c·∫£ detections t·ª´ m·ªçi window ƒë·ªÉ k·∫øt h·ª£p sau
    all_window_detections = []
    all_window_metadata = []
    
    # C√°c bi·∫øn ƒë·ªÉ t√≠nh mAP
    all_action_gt = {c: [] for c in range(NUM_CLASSES)}
    all_action_preds = {c: [] for c in range(NUM_CLASSES)}
    
    # Bi·∫øn ƒë·ªÉ t√≠nh F1 th√¥ng th∆∞·ªùng (frame-level)
    all_frame_preds = []
    all_frame_targets = []
    
    # Th√™m theo d√µi false positives
    true_positives = 0
    false_positives = 0
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validation"):
            # Unpack the batch with RGB and Pose+Velocity streams
            frames, pose_data, hand_data, action_masks, start_masks, end_masks, metadata = batch
            
            frames = frames.to(device)
            pose_data = pose_data.to(device)
            action_masks = action_masks.to(device)
            start_masks = start_masks.to(device)
            end_masks = end_masks.to(device)
            
            # Forward pass with mixed precision
            with autocast(enabled=USE_MIXED_PRECISION):
                # Forward pass v·ªõi RGB v√† Pose+Velocity
                predictions = model(frames, pose_data)
                
                # Ki·ªÉm tra predictions
                if 'classification' in predictions:
                    class_logits = predictions['classification']
                    print(f"Val Classification logits: min={class_logits.min().item():.4f}, max={class_logits.max().item():.4f}")
                
                # Calculate loss
                targets = {
                    'action_masks': action_masks,
                    'start_masks': start_masks,
                    'end_masks': end_masks
                }
                
                loss_dict = criterion(predictions, targets)
                loss = loss_dict['total']
            
            # Update metrics
            val_loss += loss.item()
            
            # Post-process ƒë·ªÉ l·∫•y action segments
            if 'classification' in predictions:
                # For TwoStreamActionNet, use classification scores
                action_probs = torch.sigmoid(predictions['classification']).unsqueeze(1).repeat(1, WINDOW_SIZE, 1)
                start_probs = torch.sigmoid(predictions['start_logits'].unsqueeze(-1).repeat(1, 1, NUM_CLASSES))
                end_probs = torch.sigmoid(predictions['end_logits'].unsqueeze(-1).repeat(1, 1, NUM_CLASSES))
            else:
                # For TemporalActionDetector, use action_scores
                action_probs = torch.sigmoid(predictions['action_scores'])
                start_probs = torch.sigmoid(predictions['start_scores'])
                end_probs = torch.sigmoid(predictions['end_scores'])
            
            # Debug raw probability distributions
            debug_raw_predictions(action_probs, start_probs, end_probs)
            
            # Custom post-process function v·ªõi threshold ri√™ng cho t·ª´ng l·ªõp
            batch_detections = custom_post_process(
                model, 
                action_probs,
                start_probs,
                end_probs,
                class_thresholds=CLASS_THRESHOLDS,
                boundary_threshold=BOUNDARY_THRESHOLD,
                nms_threshold=NMS_THRESHOLD
            )
            
            if DEBUG_DETECTION:
                debug_detection_stats(batch_detections, frames.shape[0], metadata)
            
            # Process each sample in batch
            for i, (detections, meta) in enumerate(zip(batch_detections, metadata)):
                window_size = frames.shape[2]  # Temporal dimension
                video_id = meta['video_id']
                start_idx = meta['start_idx']
                all_window_detections.append(detections)
                all_window_metadata.append(meta)
                
                # Extract ground truth segments from annotations
                for anno in meta['annotations']:
                    action_id = anno['action_id']
                    gt_start = anno['start_frame']
                    gt_end = anno['end_frame']
                    
                    # Add to GT segments v·ªõi window-relative coordinates
                    all_action_gt[action_id].append((gt_start, gt_end))
                
                # Process detections with window-relative coordinates
                for det in detections:
                    action_id = det['action_id']
                    # KH√îNG th√™m start_idx ƒë·ªÉ tr√°nh coordinate mismatch
                    start_frame = det['start_frame']
                    end_frame = det['end_frame']
                    confidence = det['confidence']
                    
                    # ƒê·∫£m b·∫£o end_frame > start_frame
                    if end_frame <= start_frame:
                        end_frame = start_frame + 1
                    
                    # Add to predictions
                    all_action_preds[action_id].append({
                        'segment': (start_frame, end_frame),
                        'score': confidence
                    })
                
                # Process frame-level predictions for F1 score
                processed_preds, processed_targets = process_for_evaluation(
                    detections,
                    meta['annotations'],
                    action_masks[i].cpu(),
                    window_size
                )
                
                all_frame_preds.extend(processed_preds)
                all_frame_targets.extend(processed_targets)
    
    merged_video_detections = merge_cross_window_detections(
    all_window_detections, 
    all_window_metadata,
    iou_threshold=0.2,
    confidence_threshold=0.15
    )
    merged_video_detections = resolve_cross_class_overlaps(merged_video_detections)
    # T√≠nh F1 d·ª±a tr√™n frame-level predictions
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_frame_targets, all_frame_preds, average='macro', zero_division=0
    )
    
    merged_all_action_preds = {c: [] for c in range(NUM_CLASSES)}

    for video_dets in merged_video_detections.values():
        for det in video_dets:
            merged_all_action_preds[det['action_id']].append({
                'segment': (det['start_frame'], det['end_frame']),
                'score': det['confidence']
            })
    # T·∫°o dictionary ƒë·ªÉ l∆∞u ground truth to√†n c·ª•c theo video_id v√† action_id
    global_action_gt = defaultdict(lambda: defaultdict(list))

    # Extract ground truth segments from annotations
    for i, meta in enumerate(all_window_metadata):
        video_id = meta['video_id']
        start_idx = meta['start_idx']  # Window's start index in global coordinates
        
        for anno in meta['annotations']:
            action_id = anno['action_id']
            # Chuy·ªÉn ground truth sang t·ªça ƒë·ªô global
            global_gt_start = start_idx + anno['start_frame']
            global_gt_end = start_idx + anno['end_frame']
            
            # Th√™m v√†o ground truth to√†n c·ª•c
            global_action_gt[video_id][action_id].append((global_gt_start, global_gt_end))

    # Chuy·ªÉn ƒë·ªïi th√†nh format t∆∞∆°ng th√≠ch v·ªõi h√†m calculate_mAP
    all_action_gt_global = {c: [] for c in range(NUM_CLASSES)}
    for video_id, actions in global_action_gt.items():
        for action_id, segments in actions.items():
            # Lo·∫°i b·ªè c√°c segments tr√πng l·∫∑p
            unique_segments = list(set(segments))
            all_action_gt_global[action_id].extend(unique_segments)

    for c in range(NUM_CLASSES):
        print(f"Class {c} - Global GT count: {len(all_action_gt_global[c])}")
        if all_action_gt_global[c]:
            print(f"  Sample GT: {all_action_gt_global[c][0]}")
        if merged_all_action_preds[c]:
            print(f"  Sample Pred: {merged_all_action_preds[c][0]}")

    # Tr·ª±c quan h√≥a k·∫øt qu·∫£ merger
    for video_id, detections in merged_video_detections.items():
        print(f"\nVideo {video_id}: {len(detections)} detections sau khi k·∫øt h·ª£p")
        detections = sorted(detections, key=lambda x: (x['action_id'], x['start_frame']))
    
    # S·ª≠ d·ª•ng merged_all_action_preds thay v√¨ all_action_preds
    mAP = calculate_mAP(all_action_gt_global, merged_all_action_preds)

    merged_all_frame_preds = []
    merged_all_frame_targets = []

    # Thu th·∫≠p t·∫•t c·∫£ frames t·ª´ c√°c video
    for video_id, detections in merged_video_detections.items():
        # T√¨m ƒë·ªô d√†i video (l·∫•y frame cu·ªëi c√πng t·ª´ detections ho·∫∑c ground truth)
        max_frame = 0
        for det in detections:
            max_frame = max(max_frame, det['end_frame'])
        
        for c in range(NUM_CLASSES):
            if video_id in global_action_gt and c in global_action_gt[video_id]:
                for start, end in global_action_gt[video_id][c]:
                    max_frame = max(max_frame, end)
        
        # T·∫°o m·∫£ng frame targets v√† predictions cho video n√†y
        video_length = max_frame + 1
        video_targets = np.zeros((video_length, NUM_CLASSES), dtype=int)
        video_preds = np.zeros((video_length, NUM_CLASSES), dtype=int)
        
        # L·∫•p ƒë·∫ßy ground truth
        if video_id in global_action_gt:
            for c, segments in global_action_gt[video_id].items():
                for start, end in segments:
                    for t in range(start, end):
                        if t < video_length:
                            video_targets[t, c] = 1
        
        # L·∫•p ƒë·∫ßy predictions
        for det in detections:
            c = det['action_id']
            start = det['start_frame']
            end = det['end_frame']
            for t in range(start, end):
                if t < video_length:
                    video_preds[t, c] = 1
        
        # Chuy·ªÉn sang d·∫°ng flatten ƒë·ªÉ t√≠nh F1
        for t in range(video_length):
            for c in range(NUM_CLASSES):
                merged_all_frame_targets.append(video_targets[t, c])
                merged_all_frame_preds.append(video_preds[t, c])

    # T√≠nh global F1
    merged_precision, merged_recall, merged_f1, _ = precision_recall_fscore_support(
        merged_all_frame_targets, merged_all_frame_preds, average='macro', zero_division=0
    )

    print(f"\n--- F1 Metrics ---")
    print(f"Window-level F1: {f1:.4f} (Precision: {precision:.4f}, Recall: {recall:.4f})")
    print(f"Global F1 after merge: {merged_f1:.4f} (Precision: {merged_precision:.4f}, Recall: {merged_recall:.4f})")

    # T√≠nh F1 cho t·ª´ng l·ªõp ri√™ng bi·ªát
    merged_class_f1 = []
    for c in range(NUM_CLASSES):
        class_targets = [merged_all_frame_targets[i] for i in range(len(merged_all_frame_targets)) 
                        if i % NUM_CLASSES == c]
        class_preds = [merged_all_frame_preds[i] for i in range(len(merged_all_frame_preds))
                    if i % NUM_CLASSES == c]
        
        if sum(class_targets) > 0:  # Ch·ªâ t√≠nh F1 cho c√°c l·ªõp c√≥ ground truth
            _, _, class_f1, _ = precision_recall_fscore_support(
                class_targets, class_preds, average='binary', zero_division=0
            )
            merged_class_f1.append(class_f1)
            print(f"Class {c} F1: {class_f1:.4f}")



        # T√≠nh segment accuracy (d√πng IoU=0.5)
    total_correct = 0
    total_global_gt_segments = sum(len(all_action_gt_global[c]) for c in range(NUM_CLASSES))
    total_pred = sum(len(merged_all_action_preds[c]) for c in range(NUM_CLASSES))

    for c in range(NUM_CLASSES):
        gt_matched = [False] * len(all_action_gt_global[c])
        for pred in merged_all_action_preds[c]:
            best_iou = 0
            best_idx = -1
            for i, gt in enumerate(all_action_gt_global[c]):
                if not gt_matched[i]:
                    iou = calculate_temporal_iou(pred['segment'], gt)
                    if iou > best_iou:
                        best_iou = iou
                        best_idx = i
            
            if best_iou >= 0.5 and best_idx >= 0:
                total_correct += 1
                gt_matched[best_idx] = True


    # T√≠nh trung b√¨nh ƒë·ªô d√†i segment tr∆∞·ªõc v√† sau khi k·∫øt h·ª£p
    avg_length_before = np.mean([det['end_frame'] - det['start_frame'] for dets in all_window_detections for det in dets] if any(all_window_detections) else [0])
    avg_length_after = np.mean([det['end_frame'] - det['start_frame'] for dets in merged_video_detections.values() for det in dets] if any(merged_video_detections.values()) else [0])
    print(f"Avg length: {avg_length_before:.2f} ‚Üí {avg_length_after:.2f} frames")
    
    # S·ª¨A L·∫†I D√íNG PRINT THEO √ù NGHƒ®A M·ªöI
    total_merged_predictions = sum(len(dets) for dets in merged_video_detections.values())
    print(f"Total Segments: Detected={total_merged_predictions} / GroundTruth={total_global_gt_segments}")

    # (C√≥ th·ªÉ gi·ªØ l·∫°i d√≤ng ƒë·∫øm d·ª± ƒëo√°n d√†i n·∫øu mu·ªën so s√°nh ri√™ng)
    # long_actions_detected = sum(1 for det in merged_video_detections.values() for d in det if d['end_frame']-d['start_frame'] > 32)
    
    # Calculate AP cho t·ª´ng l·ªõp
    class_ap_dict = {}
    print("\n--- mAP by class ---")
    
    for c in range(NUM_CLASSES):
        class_ap = calculate_class_mAP(all_action_gt_global[c], merged_all_action_preds[c])
        class_ap_dict[c] = class_ap
        num_gt = len(all_action_gt_global[c])
        num_pred = len(merged_all_action_preds[c])
        print(f"Class {c}: AP={class_ap:.4f} (GT={num_gt}, Pred={num_pred})")
    
    # T√≠nh s·ªë l∆∞·ª£ng false positives v√† true positives
    for pred, target in zip(all_frame_preds, all_frame_targets):
        if pred == 1 and target == 1:
            true_positives += 1
        elif pred == 1 and target == 0:
            false_positives += 1
    
    print("\n--- Segment-level F1 at different IoU thresholds ---")
    iou_thresholds = [0.1, 0.25, 0.5]
    avg_f1_scores = {} # Store average F1 for returning
    
    for iou in iou_thresholds:
        all_class_f1 = []
        for c in range(NUM_CLASSES):
            # Check if GT exists for this class to avoid division by zero or misleading F1=0
            if len(all_action_gt_global[c]) == 0: 
                # If no GT, F1 is undefined or arguably 0 if preds exist, 1 if no preds either. 
                # For averaging, skipping might be best unless defined otherwise.
                # Let's skip for averaging to avoid skewing.
                continue # Skip this class if no ground truth
            
            # T√≠nh F1 ·ªü ng∆∞·ª°ng IoU c·ª• th·ªÉ
            _, _, class_f1 = calculate_f1_at_iou(all_action_gt_global[c], merged_all_action_preds[c], iou)
            all_class_f1.append(class_f1)
            print(f"Class {c} - F1@{iou:.2f}: {class_f1:.4f}")
        
        avg_f1 = np.mean(all_class_f1) if all_class_f1 else 0.0 # Handle case where no class had GT
        avg_f1_scores[f'avg_f1_iou_{iou:.2f}'.replace('.', '')] = avg_f1 # Store with a key like 'avg_f1_iou_01'
        print(f"Average F1@{iou:.2f}: {avg_f1:.4f}")

    map_mid = calculate_map_mid(all_action_gt_global, merged_all_action_preds)
    print(f"mAP@mid: {map_mid:.4f}")

    accuracy = total_correct / max(1, total_global_gt_segments) # Tr√°nh chia cho 0 n·∫øu kh√¥ng c√≥ GT
    print(f"\nSegment Accuracy@0.5: {accuracy:.4f} (Correct={total_correct}, GT={total_global_gt_segments}, Pred={total_pred})")
    
    # In th√™m v·ªÅ kh·∫£ nƒÉng ph√°t hi·ªán
    total_frames = len(all_frame_preds)
    print(f"False positives: {false_positives}/{total_frames} ({false_positives/total_frames*100:.2f}%)")

    
    # T√≠nh loss trung b√¨nh
    val_loss /= len(val_loader)
    
    # Return a dictionary of all calculated metrics
    return {
        'val_loss': val_loss,
        'mAP': mAP,
        'merged_f1': merged_f1,
        'class_aps': class_ap_dict,
        'map_mid': map_mid,
        'avg_f1_iou_010': avg_f1_scores.get('avg_f1_iou_010', 0.0),
        'avg_f1_iou_025': avg_f1_scores.get('avg_f1_iou_025', 0.0),
        'avg_f1_iou_050': avg_f1_scores.get('avg_f1_iou_050', 0.0) # Use the correct key 'avg_f1_iou_050'
    }

# Custom post-process function v·ªõi threshold ri√™ng cho t·ª´ng l·ªõp
def custom_post_process(model, action_probs, start_probs, end_probs, class_thresholds, boundary_threshold=BOUNDARY_THRESHOLD, nms_threshold=NMS_THRESHOLD, min_segment_length=MIN_SEGMENT_LENGTH):
    """Post-processing ƒë∆°n gi·∫£n h√≥a:
    1. T√¨m start/end candidates > boundary_threshold.
    2. T·∫°o t·∫•t c·∫£ c√°c c·∫∑p (start, end) h·ª£p l·ªá (ƒë√∫ng min_length).
    3. L·ªçc c√°c c·∫∑p d·ª±a tr√™n action_score trung b√¨nh > class_threshold.
    4. T√≠nh confidence k·∫øt h·ª£p.
    5. √Åp d·ª•ng NMS cho c√πng l·ªõp trong c·ª≠a s·ªï.
    (Lo·∫°i b·ªè x·ª≠ l√Ω overlap kh√°c l·ªõp trong c·ª≠a s·ªï)
    """
    batch_size, seq_len, num_classes = action_probs.shape
    all_detections_batch = []
    
    for b in range(batch_size):
        detections_window = [] # Detections cho c·ª≠a s·ªï hi·ªán t·∫°i
        
        for c in range(num_classes):
            action_score_c = action_probs[b, :, c]  # (T,)
            start_score_c = start_probs[b, :, c]    # (T,)
            end_score_c = end_probs[b, :, c]        # (T,)
            class_threshold_c = class_thresholds[c]

            # 1. T√¨m start/end candidates
            start_indices = torch.where(start_score_c > boundary_threshold)[0]
            end_indices = torch.where(end_score_c > boundary_threshold)[0]

            # === ADD DEBUG FOR CLASS 2 ===
            if c == 2:
                print(f"DEBUG Class 2 (window {b}): Max Start Score = {start_score_c.max().item():.4f}, Max End Score = {end_score_c.max().item():.4f}")
                print(f"DEBUG Class 2 (window {b}): Num Start Indices (> {boundary_threshold}) = {len(start_indices)}, Num End Indices (> {boundary_threshold}) = {len(end_indices)}")
            # === END DEBUG ===

            if len(start_indices) == 0 or len(end_indices) == 0:
                continue
            
            proposals_class_c = []
            # 2. T·∫°o t·∫•t c·∫£ c√°c c·∫∑p (start, end) h·ª£p l·ªá
            for start_idx_tensor in start_indices:
                start_idx = start_idx_tensor.item()
                # Ch·ªâ x√©t end_indices sau start_idx
                valid_end_indices = end_indices[end_indices > start_idx]

                for end_idx_tensor in valid_end_indices:
                    end_idx = end_idx_tensor.item()

                    # Ki·ªÉm tra ƒë·ªô d√†i t·ªëi thi·ªÉu
                    if (end_idx - start_idx) >= min_segment_length:
                        # 3. L·ªçc d·ª±a tr√™n action score trung b√¨nh
                        segment_action_score = action_score_c[start_idx:end_idx].mean().item()

                        if segment_action_score > class_threshold_c:
                            # 4. T√≠nh confidence k·∫øt h·ª£p
                            start_conf = start_score_c[start_idx].item()
                            # L·∫•y ƒëi·ªÉm end c·ªßa frame cu·ªëi c√πng TRONG segment (end_idx l√† exclusive)
                            # ƒê·∫£m b·∫£o end_idx-1 kh√¥ng nh·ªè h∆°n start_idx
                            effective_end_idx = max(start_idx, end_idx - 1)
                            end_conf = end_score_c[effective_end_idx].item()

                            confidence = (segment_action_score + start_conf + end_conf) / 3.0

                            proposals_class_c.append({
                        'action_id': c,
                                'start_frame': start_idx,
                                'end_frame': end_idx, # end_idx l√† exclusive
                                'confidence': confidence
                            })

            # Th√™m proposals c·ªßa l·ªõp n√†y v√†o danh s√°ch chung c·ªßa window
            detections_window.extend(proposals_class_c)

        # 5. √Åp d·ª•ng NMS cho T·∫§T C·∫¢ detections trong window (ch·ªâ lo·∫°i b·ªè c√πng l·ªõp)
        # S·∫Øp x·∫øp tr∆∞·ªõc khi v√†o NMS ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n
        detections_window = sorted(detections_window, key=lambda x: x['confidence'], reverse=True)
        detections_window_nms = nms(detections_window, nms_threshold) # nms ch·ªâ x·ª≠ l√Ω overlap C√ôNG L·ªöP

        # KH√îNG c√≤n x·ª≠ l√Ω overlap kh√°c l·ªõp ·ªü ƒë√¢y
        # KH√îNG c√≤n g·ªçi validate_detections ·ªü ƒë√¢y

        all_detections_batch.append(detections_window_nms)

    return all_detections_batch

def nms(detections, threshold):
    """Non-maximum suppression for action detections (cho c√πng m·ªôt l·ªõp)"""
    if not detections:
        return []
    
    # Sort by confidence (descending)
    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
    
    # Apply NMS
    keep = []
    while detections:
        current = detections.pop(0)
        keep.append(current)
        
        # Remove overlapping detections with IoU > threshold
        detections = [
            d for d in detections if 
            calculate_temporal_iou(
                (current['start_frame'], current['end_frame']),
                (d['start_frame'], d['end_frame'])
            ) <= threshold
        ]
    
    return keep

def calculate_class_mAP(gt_segments, pred_segments, iou_threshold=0.5):
    """Calculate AP for a single class"""
    # Handle edge cases
    if len(gt_segments) == 0:
        return 0.0  # No ground truth -> cannot compute AP
    
    if len(pred_segments) == 0:
        return 0.0  # No predictions -> precision is 0
    
    # Sort predictions by confidence
    pred_segments = sorted(pred_segments, key=lambda x: x['score'], reverse=True)
    
    # Mark GT segments as detected or not
    gt_detected = [False] * len(gt_segments)
    
    # Arrays for AP calculation
    y_true = []
    y_score = []
    
    # For each prediction
    for pred in pred_segments:
        pred_segment = pred['segment']
        score = pred['score']
        
        # Add score to y_score
        y_score.append(score)
        
        # Find best matching GT segment
        max_iou = 0
        max_idx = -1
        
        for i, gt_segment in enumerate(gt_segments):
            if gt_detected[i]:
                continue  # Skip already detected GT segments
            
            iou = calculate_temporal_iou(pred_segment, gt_segment)
            if iou > max_iou:
                max_iou = iou
                max_idx = i
        
        # Match if IoU >= threshold
        if max_iou >= iou_threshold and max_idx >= 0:
            gt_detected[max_idx] = True
            y_true.append(1)  # True positive
        else:
            y_true.append(0)  # False positive
    
    # Calculate AP
    if sum(y_true) > 0:
        ap = average_precision_score(y_true, y_score)
    else:
        ap = 0.0
    
    return ap

def calculate_map_mid(all_action_gt, all_action_preds):
    """Calculate mAP with midpoint criterion instead of IoU"""
    aps = []
    
    for action_id in range(NUM_CLASSES):
        gt_segments = all_action_gt[action_id]
        pred_segments = all_action_preds[action_id]
        
        # Skip if no ground truth
        if len(gt_segments) == 0:
            continue
        
        # Sort predictions by confidence
        pred_segments = sorted(pred_segments, key=lambda x: x['score'], reverse=True)
        
        # Arrays for AP calculation
        y_true = []
        y_score = []
        
        # ƒê√°nh d·∫•u GT segments ƒë√£ ƒë∆∞·ª£c ph√°t hi·ªán
        gt_detected = [False] * len(gt_segments)
        
        for pred in pred_segments:
            pred_segment = pred['segment']
            score = pred['score']
            
            # T√≠nh midpoint
            pred_mid = (pred_segment[0] + pred_segment[1]) / 2
            
            # Th√™m score
            y_score.append(score)
            
            # Ki·ªÉm tra xem midpoint c√≥ n·∫±m trong b·∫•t k·ª≥ GT n√†o
            is_correct = False
            for i, gt_segment in enumerate(gt_segments):
                if not gt_detected[i] and gt_segment[0] <= pred_mid <= gt_segment[1]:
                    gt_detected[i] = True
                    is_correct = True
                    break
            
            y_true.append(1 if is_correct else 0)
        
        # T√≠nh AP
        if sum(y_true) > 0:  # N·∫øu c√≥ √≠t nh·∫•t 1 detection ƒë√∫ng
            ap = average_precision_score(y_true, y_score)
        else:
            ap = 0.0
        
        aps.append(ap)
    
    # T√≠nh mAP
    return np.mean(aps) if aps else 0.0

def calculate_mAP(all_action_gt, all_action_preds, iou_thresholds=[0.3, 0.5, 0.7]):
    """Calculate mean Average Precision across classes and IoU thresholds"""
    # Calculate AP for each class and IoU threshold
    aps = []
    
    for action_id in range(NUM_CLASSES):
        gt_segments = all_action_gt[action_id]
        pred_segments = all_action_preds[action_id]
        
        # Skip if no ground truth
        if len(gt_segments) == 0:
            continue
        
        # Calculate AP for each IoU threshold
        class_aps = []
        for iou_threshold in iou_thresholds:
            ap = calculate_class_mAP(gt_segments, pred_segments, iou_threshold)
            class_aps.append(ap)
        
        # Average AP across IoU thresholds
        aps.append(np.mean(class_aps))
    
    # Calculate mAP
    return np.mean(aps) if aps else 0.0

def process_for_evaluation(detections, gt_annotations, action_masks, window_size):
    """Process predictions and ground truth for evaluation metrics"""
    processed_preds = []
    processed_targets = []
    
    # Process each frame
    for t in range(window_size):
        for c in range(NUM_CLASSES):
            # Check if frame t belongs to class c in ground truth
            is_gt_action = False
            for anno in gt_annotations:
                if anno['action_id'] == c and anno['start_frame'] <= t < anno['end_frame']:
                    is_gt_action = True
                    break
            
            # Check if frame t belongs to class c in predictions
            is_pred_action = False
            for det in detections:
                if det['action_id'] == c and det['start_frame'] <= t < det['end_frame']:
                    is_pred_action = True
                    break
            
            processed_preds.append(1 if is_pred_action else 0)
            processed_targets.append(1 if is_gt_action else 0)
    
    return processed_preds, processed_targets

# ====== Main Function ======
def main():
    """Main training function"""
    print(f"Using device: {DEVICE}")
    
    # Log GPU memory stats
    if torch.cuda.is_available():
        set_seed(42) 
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        print(f"Mixed precision: {'Enabled' if USE_MIXED_PRECISION else 'Disabled'}")
        print(f"Detection thresholds: Boundary={BOUNDARY_THRESHOLD}")
        print(f"Class-specific thresholds: {CLASS_THRESHOLDS}")
        print(f"Loss weights: Action={ACTION_WEIGHT}, Start={START_WEIGHT}, End={END_WEIGHT}")
        print(f"Class 2 weight: {2.0}x, Class 3 weight: {2.0}x")
        print(f"Learning rate: {LR}, Peak warmup: {LR*WARMUP_FACTOR}, Weight decay: {WEIGHT_DECAY}")
        print(f"Gradient clipping: {MAX_GRAD_NORM}")
        print(f"Warmup epochs: {WARMUP_EPOCHS}")
        print(f"MIN_CONFIDENT_RATIO: {MIN_CONFIDENT_RATIO}")
    
    # Get dataloaders
    train_loader = get_train_loader(batch_size=BATCH_SIZE)
    val_loader = get_val_loader(batch_size=BATCH_SIZE)  # Use validation loader instead of test
    
    # Initialize model - use TemporalActionDetector for multi-stream processing
    model = TemporalActionDetector(num_classes=NUM_CLASSES, window_size=WINDOW_SIZE)
    
    # Chuy·ªÉn model to device tr∆∞·ªõc
    model = model.to(DEVICE)
    
    # Initialize optimizer and scheduler
    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)
    
    # Thay ƒë·ªïi scheduler t·ª´ CosineAnnealingLR sang ReduceLROnPlateau
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',        # Gi·∫£m LR khi metric gi·∫£m
        factor=0.2,        # Gi·∫£m LR 50% m·ªói l·∫ßn
        patience=3,        # ƒê·ª£i 3 epochs kh√¥ng c·∫£i thi·ªán
        min_lr=1e-6,       # LR t·ªëi thi·ªÉu
        verbose=True       # In th√¥ng b√°o khi LR thay ƒë·ªïi
    )
    
    # Kh·ªüi t·∫°o training state
    start_epoch = 0
    best_map = 0
    
    # Resume training n·∫øu RESUME_TRAINING=True v√† file checkpoint t·ªìn t·∫°i
    if RESUME_TRAINING and os.path.exists(RESUME_CHECKPOINT):
        print(f"Resuming training from checkpoint: {RESUME_CHECKPOINT}")
        checkpoint = torch.load(RESUME_CHECKPOINT, map_location=DEVICE)
        
        # Load model state
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # ƒê·∫£m b·∫£o t·∫•t c·∫£ params ƒë·ªÅu ·ªü GPU sau khi load state dict
        model = model.to(DEVICE)
        
        # Kh·ªüi t·∫°o optimizer m·ªõi tr√™n c√πng device
        optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)
        
        # Load optimizer state
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # Chuy·ªÉn t·∫•t c·∫£ state tensors c·ªßa optimizer l√™n GPU
        for state in optimizer.state.values():
            for k, v in state.items():
                if isinstance(v, torch.Tensor):
                    state[k] = v.to(DEVICE)
        
        # Kh·ªüi t·∫°o scheduler m·ªõi
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.2,
            patience=3,
            min_lr=1e-6,
            verbose=True
        )
        # Load scheduler state if available in checkpoint
        if 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            print("Loaded scheduler state from checkpoint.")
        else:
            print("Scheduler state not found in checkpoint, initializing new scheduler.")
        
        # Set training state
        start_epoch = checkpoint['epoch']
        best_map = checkpoint['val_map']
        
        print(f"Loaded checkpoint from epoch {start_epoch} with mAP: {best_map:.4f}")
    else:
        print("No checkpoint found, starting from scratch")
    
    # Print model summary
    print(f"Model: TemporalActionDetector with {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters")
    print(f"Streams: RGB + Pose with Velocity (198 dims, b·ªè Hand stream)")
    print(f"Scheduler: ReduceLROnPlateau (factor=0.5, patience=3, min_lr=1e-6)")
    
    # Initialize loss function v·ªõi class weights v√† label smoothing
    criterion = ActionDetectionLoss(action_weight=ACTION_WEIGHT, start_weight=START_WEIGHT, 
                                   end_weight=END_WEIGHT, label_smoothing=0.1)
    
    
    # Train model
    try:
        best_map = train(model, train_loader, val_loader, criterion, optimizer, scheduler, EPOCHS, DEVICE, 
                          start_epoch=start_epoch, best_map=best_map)
        print(f"\n‚úÖ Training complete! Best validation mAP: {best_map:.4f}")
        print(f"Best model saved to {CHECKPOINT}")
        
        # Final evaluation on test set if requested
        if FINAL_EVALUATION:
            print("\n=== Final Evaluation on Test Set ===")
            # Load best model
            checkpoint = torch.load(CHECKPOINT, map_location=DEVICE)
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()
            
            # Get test loader
            test_loader = get_test_loader(batch_size=BATCH_SIZE)
            
            # Evaluate on test set
            test_loss, test_map, test_f1, test_class_ap_dict = evaluate(model, test_loader, criterion, DEVICE)
            
            print(f"Test Loss: {test_loss:.4f}, Test mAP: {test_map:.4f}, Test F1: {test_f1:.4f}")
            print(f"Test Class AP: {', '.join([f'C{c}={test_class_ap_dict[c]:.4f}' for c in range(NUM_CLASSES)])}")
            
            # Save test results
            test_results = {
                'test_map': test_map,
                'test_f1': test_f1,
                'test_loss': test_loss,
                'class_aps': test_class_ap_dict
            }
            
            with open(os.path.join(LOG_DIR, 'test_results.json'), 'w') as f:
                json.dump(test_results, f, indent=2)
                
    except Exception as e:
        print(f"\n‚ùå Error during training: {str(e)}")
        import traceback
        traceback.print_exc()
        # Try to free up memory
        torch.cuda.empty_cache()
        raise e

if __name__ == "__main__":
    torch.cuda.empty_cache()  # Clear cache before starting
    main() 
